{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbcf696f-c4c3-42be-b9a4-1a69bcc65e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "LINE_class_labels = ['ape', 'benchvise', 'bowl', 'can', 'cat', 'cup',\n",
    "                    'driller', 'duck', 'glue', 'holepuncher', 'iron',\n",
    "                    'lamp', 'phone', 'cam', 'eggbox']\n",
    "LINE_class_nums = len(LINE_class_labels)\n",
    "\n",
    "LINE_ROOT = os.path.join(DATA_ROOT, 'lm', 'train_pbr')\n",
    "class COCOSingleDatasetBase(ObjectDetectionDatasetBase):\n",
    "    def __init__(self, coco_dir, focus, ignore=None, transform=None, target_transform=None, augmentation=None, class_labels=None):\n",
    "        \"\"\"\n",
    "        :param coco_dir: str, coco directory path above 'annotations' and 'images'\n",
    "                e.g.) coco_dir = '~~~~/coco2007/trainval'\n",
    "        :param focus: str or str, directory name under images\n",
    "                e.g.) focus = 'train2014'\n",
    "        :param ignore: target_transforms.Ignore\n",
    "        :param transform: instance of transforms\n",
    "        :param target_transform: instance of target_transforms\n",
    "        :param augmentation:  instance of augmentations\n",
    "        :param class_labels: None or list or tuple, if it's None use VOC_class_labels\n",
    "        \"\"\"\n",
    "        super().__init__(ignore=ignore, transform=transform, target_transform=target_transform, augmentation=augmentation)\n",
    "\n",
    "        self._coco_dir = coco_dir\n",
    "        self._focus = focus\n",
    "\n",
    "        self._class_labels = _check_ins('class_labels', class_labels, (list, tuple), allow_none=True)\n",
    "        if self._class_labels is None:\n",
    "            self._class_labels = LINE_class_labels\n",
    "\n",
    "        self._annopath = os.path.join(self._coco_dir, self._focus, 'scene_gt_coco_modal.json')\n",
    "        if os.path.exists(self._annopath):\n",
    "            self._coco = COCO(self._annopath)\n",
    "        else:\n",
    "            raise FileNotFoundError('json: {} was not found'.format('instances_' + self._focus + '.json'))\n",
    "\n",
    "\n",
    "        # remove no annotation image\n",
    "        self._imageids = list(self._coco.imgToAnns.keys())\n",
    "\n",
    "    @property\n",
    "    def class_nums(self):\n",
    "        return len(self._class_labels)\n",
    "    @property\n",
    "    def class_labels(self):\n",
    "        return self._class_labels\n",
    "\n",
    "    def _jpgpath(self, filename,forcus):\n",
    "        \"\"\"\n",
    "        :param filename: path containing .jpg\n",
    "        :return: path of jpg\n",
    "        \"\"\"\n",
    "        return os.path.join(LINE_ROOT, forcus,filename) #self._coco_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._imageids)\n",
    "\n",
    "    \"\"\"\n",
    "    Detail of contents in voc > https://towardsdatascience.com/coco-data-format-for-object-detection-a4c5eaf518c5\n",
    "\n",
    "    VOC bounding box (xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    def _get_image(self, index):\n",
    "        \"\"\"\n",
    "        :param index: int\n",
    "        :return:\n",
    "            rgb image(ndarray)\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        self._coco.loadImgs(self._imageids[index]): list of dict, contains;\n",
    "            license: int\n",
    "            file_name: str\n",
    "            coco_url: str\n",
    "            height: int\n",
    "            width: int\n",
    "            date_captured: str\n",
    "            flickr_url: str\n",
    "            id: int\n",
    "        \"\"\"\n",
    "        filename = self._coco.loadImgs(self._imageids[index])[0]['file_name']\n",
    "        img = cv2.imread(self._jpgpath(filename))\n",
    "        # pytorch's image order is rgb\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        return img.astype(np.float32)\n",
    "\n",
    "    def _get_target(self, index):\n",
    "        \"\"\"\n",
    "        :param index: int\n",
    "        :return:\n",
    "            list of bboxes, list of bboxes' label index, list of flags([difficult, truncated,...])\n",
    "        \"\"\"\n",
    "        linds = []\n",
    "        bboxes = []\n",
    "        flags = []\n",
    "\n",
    "        # anno_ids is list\n",
    "        anno_ids = self._coco.getAnnIds(self._imageids[index])\n",
    "\n",
    "        # annos is list of dict\n",
    "        annos = self._coco.loadAnns(anno_ids)\n",
    "        for anno in annos:\n",
    "            \"\"\"\n",
    "            anno's  keys are;\n",
    "                segmentation: list of float\n",
    "                area: float\n",
    "                iscrowd: int, 0 or 1\n",
    "                image_id: int\n",
    "                bbox: list of float, whose length is 4\n",
    "                category_id: int\n",
    "                id: int\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            self._coco.loadCats(anno['category_id']) is list of dict, contains;\n",
    "                supercategory: str\n",
    "                id: int\n",
    "                name: str\n",
    "            \"\"\"\n",
    "            cat = self._coco.loadCats(anno['category_id'])[0]\n",
    "\n",
    "            linds.append(self.class_labels.index(cat['name']))\n",
    "\n",
    "            # bbox = [xmin, ymin, w, h]\n",
    "            xmin, ymin, w, h = anno['bbox']\n",
    "            # convert to corners\n",
    "            xmax, ymax = xmin + w, ymin + h\n",
    "            bboxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            \"\"\"\n",
    "            flag = {}\n",
    "            keys = ['iscrowd']\n",
    "            for key in keys:\n",
    "                if key in anno.keys():\n",
    "                    flag[key] = anno[key] == 1\n",
    "                else:\n",
    "                    flag[key] = False\n",
    "            flags.append(flag)\n",
    "            \"\"\"\n",
    "            flags.append({'difficult': anno['iscrowd'] == 1})\n",
    "\n",
    "        return np.array(bboxes, dtype=np.float32), np.array(linds, dtype=np.float32), flags\n",
    "\n",
    "\n",
    "class COCOMultiDatasetBase(Compose):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        :param datasets: tuple of Dataset\n",
    "        :param kwargs:\n",
    "            :param ignore:\n",
    "            :param transform:\n",
    "            :param target_transform:\n",
    "            :param augmentation:\n",
    "        \"\"\"\n",
    "        super().__init__(datasets=(), **kwargs)\n",
    "\n",
    "        coco_dir = _check_ins('coco_dir', kwargs.pop('coco_dir'), (tuple, list, str))\n",
    "        focus = _check_ins('focus', kwargs.pop('focus'), (tuple, list, str))\n",
    "\n",
    "        if isinstance(coco_dir, str) and isinstance(focus, str):\n",
    "            datasets = [COCOSingleDatasetBase(coco_dir, focus, **kwargs)]\n",
    "            lens = [len(datasets[0])]\n",
    "\n",
    "        elif isinstance(coco_dir, (list, tuple)) and isinstance(focus, (list, tuple)):\n",
    "            if len(coco_dir) != len(focus):\n",
    "                raise ValueError('coco_dir and focus must be same length, but got {}, {}'.format(len(coco_dir), len(focus)))\n",
    "\n",
    "            datasets = [COCOSingleDatasetBase(cdir, f, **kwargs) for cdir, f in zip(coco_dir, focus)]\n",
    "            lens = [len(d) for d in datasets]\n",
    "        else:\n",
    "            raise ValueError('Invalid coco_dir and focus combination')\n",
    "\n",
    "        self.datasets = datasets\n",
    "        self.lens = lens\n",
    "        self._class_labels = datasets[0].class_labels\n",
    "\n",
    "\n",
    "class COCO2014_TrainDataset(COCOSingleDatasetBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(DATA_ROOT + '/coco/coco2014/trainval', 'train2014', **kwargs)\n",
    "\n",
    "class COCO2014_ValDataset(COCOSingleDatasetBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(DATA_ROOT + '/coco/coco2014/trainval', 'val2014', **kwargs)\n",
    "\n",
    "class COCO2014_TrainValDataset(COCOMultiDatasetBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(coco_dir=(DATA_ROOT + '/coco/coco2014/trainval',\n",
    "                                   DATA_ROOT + '/coco/coco2014/trainval'),\n",
    "                         focus=('train2014', 'val2014'), **kwargs)\n",
    "\n",
    "class COCO2017_TrainDataset(COCOSingleDatasetBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(DATA_ROOT + '/coco/coco2017/trainval', 'train2014', **kwargs)\n",
    "\n",
    "class COCO2017_ValDataset(COCOSingleDatasetBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(DATA_ROOT + '/coco/coco2017/trainval', 'val2014', **kwargs)\n",
    "\n",
    "class COCO2017_TrainValDataset(COCOMultiDatasetBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(coco_dir=(DATA_ROOT + '/coco/coco2017/trainval',\n",
    "                                   DATA_ROOT + '/coco/coco2017/trainval'),\n",
    "                         focus=('train2017', 'val2017'), **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6af0674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[301., 191., 355., 296.],\n",
       "        [335., 213., 433., 348.],\n",
       "        [331., 249., 335., 279.],\n",
       "        [186., 218., 259., 261.],\n",
       "        [244., 217., 277., 259.],\n",
       "        [444., 313., 471., 362.],\n",
       "        [  0., 158.,  98., 243.],\n",
       "        [390., 220., 421., 295.],\n",
       "        [316., 238., 335., 253.],\n",
       "        [471., 257., 562., 394.],\n",
       "        [411., 292., 523., 442.],\n",
       "        [402., 318., 449., 376.],\n",
       "        [334., 266., 396., 297.]], dtype=float32),\n",
       " array([ 1., 13., 10.,  2.,  6.,  0., 12.,  4.,  9.,  7., 14.,  8.,  3.],\n",
       "       dtype=float32),\n",
       " [{'difficult': False},\n",
       "  {'difficult': False},\n",
       "  {'difficult': False},\n",
       "  {'difficult': False},\n",
       "  {'difficult': False},\n",
       "  {'difficult': False},\n",
       "  {'difficult': False},\n",
       "  {'difficult': False},\n",
       "  {'difficult': False},\n",
       "  {'difficult': False},\n",
       "  {'difficult': False},\n",
       "  {'difficult': False},\n",
       "  {'difficult': False}])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annopath = os.path.join(LINE_ROOT, '000000', 'scene_gt_coco_modal.json')\n",
    "A = COCO(annopath)\n",
    "B=list(A.imgToAnns.keys())\n",
    "imageids=list(B)\n",
    "def jpgpath(filename):        \n",
    "    return os.path.join(LINE_ROOT, '000000',filename)\n",
    "\n",
    "C =jpgpath(A.loadImgs(imageids[1])[0]['file_name'])\n",
    "C\n",
    "#self._coco = COCO(self._annopath)\n",
    "#filename = loadImgs(A[index])[0]['file_name']\n",
    "class_labels = ['1', '2', '3', '4', '5',\n",
    "                    '6', '7', '8', '9', '10',\n",
    "                    '11', '12', '13', '14','15']\n",
    "class_labels = _check_ins('class_labels', class_labels, (list, tuple), allow_none=True)\n",
    "#class_labels\n",
    "#A.loadImgs(imageids[1])[0]['file_name']\n",
    "def get_target(index):\n",
    "        \"\"\"\n",
    "        :param index: int\n",
    "        :return:\n",
    "            list of bboxes, list of bboxes' label index, list of flags([difficult, truncated,...])\n",
    "        \"\"\"\n",
    "        linds = []\n",
    "        bboxes = []\n",
    "        flags = []\n",
    "\n",
    "        # anno_ids is list\n",
    "        anno_ids = A.getAnnIds(imageids[index])\n",
    "\n",
    "        # annos is list of dict\n",
    "        annos = A.loadAnns(anno_ids)\n",
    "        for anno in annos:\n",
    "            \"\"\"\n",
    "            anno's  keys are;\n",
    "                segmentation: list of float\n",
    "                area: float\n",
    "                iscrowd: int, 0 or 1\n",
    "                image_id: int\n",
    "                bbox: list of float, whose length is 4\n",
    "                category_id: int\n",
    "                id: int\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            self._coco.loadCats(anno['category_id']) is list of dict, contains;\n",
    "                supercategory: str\n",
    "                id: int\n",
    "                name: str\n",
    "            \"\"\"\n",
    "            cat = A.loadCats(anno['category_id'])[0]\n",
    "\n",
    "            linds.append(class_labels.index(cat['name']))\n",
    "\n",
    "            # bbox = [xmin, ymin, w, h]\n",
    "            xmin, ymin, w, h = anno['bbox']\n",
    "            # convert to corners\n",
    "            xmax, ymax = xmin + w, ymin + h\n",
    "            bboxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            \"\"\"\n",
    "            flag = {}\n",
    "            keys = ['iscrowd']\n",
    "            for key in keys:\n",
    "                if key in anno.keys():\n",
    "                    flag[key] = anno[key] == 1\n",
    "                else:\n",
    "                    flag[key] = False\n",
    "            flags.append(flag)\n",
    "            \"\"\"\n",
    "            flags.append({'difficult': anno['iscrowd'] == 1})\n",
    "\n",
    "        return np.array(bboxes, dtype=np.float32), np.array(linds, dtype=np.float32), flags\n",
    "\n",
    "get_target(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eab83311",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 12\u001b[0m\n\u001b[0;32m      2\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[0;32m      3\u001b[0m     [transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m300\u001b[39m)),\n\u001b[0;32m      4\u001b[0m      transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      5\u001b[0m      transforms\u001b[38;5;241m.\u001b[39mNormalize(rgb_means\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m), rgb_stds\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m))]\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m target_transform \u001b[38;5;241m=\u001b[39m target_transforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[0;32m      8\u001b[0m     [target_transforms\u001b[38;5;241m.\u001b[39mCorners2Centroids(),\n\u001b[0;32m      9\u001b[0m      target_transforms\u001b[38;5;241m.\u001b[39mOneHot(class_nums\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(LINE_class_labels), add_background\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     10\u001b[0m      target_transforms\u001b[38;5;241m.\u001b[39mToTensor()]\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCOCOMultiDatasetBase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoco_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mLINE_ROOT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfocus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m000000\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLINE_class_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 170\u001b[0m, in \u001b[0;36mCOCOMultiDatasetBase.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(coco_dir) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(focus):\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoco_dir and focus must be same length, but got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(coco_dir), \u001b[38;5;28mlen\u001b[39m(focus)))\n\u001b[1;32m--> 170\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m [COCOSingleDatasetBase(cdir, f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m cdir, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(coco_dir, focus)]\n\u001b[0;32m    171\u001b[0m     lens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[15], line 170\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(coco_dir) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(focus):\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoco_dir and focus must be same length, but got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(coco_dir), \u001b[38;5;28mlen\u001b[39m(focus)))\n\u001b[1;32m--> 170\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m [COCOSingleDatasetBase(cdir, f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m cdir, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(coco_dir, focus)]\n\u001b[0;32m    171\u001b[0m     lens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[15], line 25\u001b[0m, in \u001b[0;36mCOCOSingleDatasetBase.__init__\u001b[1;34m(self, coco_dir, focus, ignore, transform, target_transform, augmentation, class_labels)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, coco_dir, focus, ignore\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, target_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, augmentation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, class_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    :param coco_dir: str, coco directory path above 'annotations' and 'images'\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m            e.g.) coco_dir = '~~~~/coco2007/trainval'\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    :param class_labels: None or list or tuple, if it's None use VOC_class_labels\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugmentation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coco_dir \u001b[38;5;241m=\u001b[39m coco_dir\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_focus \u001b[38;5;241m=\u001b[39m focus\n",
      "Cell \u001b[1;32mIn[7], line 38\u001b[0m, in \u001b[0;36mObjectDetectionDatasetBase.__init__\u001b[1;34m(self, ignore, transform, target_transform, augmentation)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore \u001b[38;5;241m=\u001b[39m _check_ins(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, ignore, Ignore, allow_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;241m=\u001b[39m \u001b[43m_contain_ignore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugmentation \u001b[38;5;241m=\u001b[39m augmentation\n",
      "Cell \u001b[1;32mIn[5], line 74\u001b[0m, in \u001b[0;36m_contain_ignore\u001b[1;34m(target_transform)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_contain_ignore\u001b[39m(target_transform):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target_transform:\n\u001b[1;32m---> 74\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtarget_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ignore, Compose\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target_transform, Ignore):\n\u001b[0;32m     76\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_transforms.Ignore must be passed to \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from ssd_data import transforms, target_transforms, augmentations, utils\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((300, 300)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(rgb_means=(0.485, 0.456, 0.406), rgb_stds=(0.229, 0.224, 0.225))]\n",
    ")\n",
    "target_transform = target_transforms.Compose(\n",
    "    [target_transforms.Corners2Centroids(),\n",
    "     target_transforms.OneHot(class_nums=len(LINE_class_labels), add_background=True),\n",
    "     target_transforms.ToTensor()]\n",
    ")\n",
    "train_dataset = COCOMultiDatasetBase(coco_dir=[LINE_ROOT], focus=['000000'], ignore=None,\n",
    "                                                  transform=transform, target_transform=target_transform, augmentation=None,\n",
    "                                                  class_labels=LINE_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb9c62be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_dir = _check_ins('coco_dir', ['coco_dir'], (tuple, list, str))\n",
    "focus = _check_ins('focus', ['focus'], (tuple, list, str))\n",
    "len(coco_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb850d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import abc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ref > https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "torch.utils.data.Dataset is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods:\n",
    "\n",
    "__len__ so that len(dataset) returns the size of the dataset.\n",
    "__getitem__ to support the indexing such that dataset[i] can be used to get ith sample\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class _DatasetBase(Dataset):\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def class_nums(self):\n",
    "        pass\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def class_labels(self):\n",
    "        pass\n",
    "\n",
    "class ObjectDetectionDatasetBase(_DatasetBase):\n",
    "    def __init__(self, ignore=None, transform=None, target_transform=None, augmentation=None):\n",
    "        \"\"\"\n",
    "        :param ignore: target_transforms.Ignore\n",
    "        :param transform: instance of transforms\n",
    "        :param target_transform: instance of target_transforms\n",
    "        :param augmentation:  instance of augmentations\n",
    "        \"\"\"\n",
    "        #ignore, target_transform = _separate_ignore(target_transform)\n",
    "        self.ignore = _check_ins('ignore', ignore, Ignore, allow_none=True)\n",
    "        self.transform = transform\n",
    "        self.target_transform = _contain_ignore(target_transform)\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def class_nums(self):\n",
    "        pass\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def class_labels(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _get_image(self, index):\n",
    "        \"\"\"\n",
    "        :param index: int\n",
    "        :return:\n",
    "            rgb image(Tensor)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('\\'_get_image\\' must be overridden')\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _get_target(self, index):\n",
    "        \"\"\"\n",
    "        :param index: int\n",
    "        :return:\n",
    "            list of bboxes, list of bboxes' label index, list of flags([difficult, truncated])\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('\\'_get_target\\' must be overridden')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        :param index: int\n",
    "        :return:\n",
    "            img : rgb image(Tensor or ndarray)\n",
    "            targets : Tensor or ndarray of bboxes and labels [box, label]\n",
    "            = [xmin, ymin, xmamx, ymax, label index(or relu_one-hotted label)]\n",
    "            or\n",
    "            = [cx, cy, w, h, label index(or relu_one-hotted label)]\n",
    "        \"\"\"\n",
    "        img = self._get_image(index)\n",
    "        targets = self._get_target(index)\n",
    "        if len(targets) >= 3:\n",
    "            bboxes, linds, flags = targets[:3]\n",
    "            args = targets[3:]\n",
    "        else:\n",
    "            raise ValueError('ValueError: not enough values to unpack (expected more than 3, got {})'.format(len(targets)))\n",
    "        img, bboxes, linds, flags, args = self.apply_transform(img, bboxes, linds, flags, *args)\n",
    "\n",
    "        # concatenate bboxes and linds\n",
    "        if isinstance(bboxes, torch.Tensor) and isinstance(linds, torch.Tensor):\n",
    "            if linds.ndim == 1:\n",
    "                linds = linds.unsqueeze(1)\n",
    "            targets = torch.cat((bboxes, linds), dim=1)\n",
    "        else:\n",
    "            if linds.ndim == 1:\n",
    "                linds = linds[:, np.newaxis]\n",
    "            targets = np.concatenate((bboxes, linds), axis=1)\n",
    "\n",
    "        return img, targets\n",
    "\n",
    "    def apply_transform(self, img, bboxes, linds, flags, *args):\n",
    "        \"\"\"\n",
    "        IMPORTATANT: apply transform function in order with ignore, augmentation, transform and target_transform\n",
    "        :param img:\n",
    "        :param bboxes:\n",
    "        :param linds:\n",
    "        :param flags:\n",
    "        :return:\n",
    "            Transformed img, bboxes, linds, flags\n",
    "        \"\"\"\n",
    "        # To Percent mode\n",
    "        height, width, channel = img.shape\n",
    "        # bbox = [xmin, ymin, xmax, ymax]\n",
    "        # [bbox[0] / width, bbox[1] / height, bbox[2] / width, bbox[3] / height]\n",
    "        bboxes[:, 0::2] /= float(width)\n",
    "        bboxes[:, 1::2] /= float(height)\n",
    "\n",
    "        if self.ignore:\n",
    "            bboxes, linds, flags, args = self.ignore(bboxes, linds, flags, *args)\n",
    "\n",
    "        if self.augmentation:\n",
    "            img, bboxes, linds, flags, args = self.augmentation(img, bboxes, linds, flags, *args)\n",
    "\n",
    "        if self.transform:\n",
    "            img, bboxes, linds, flag, args = self.transform(img, bboxes, linds, flags, *args)\n",
    "\n",
    "        if self.target_transform:\n",
    "            bboxes, linds, flags, args = self.target_transform(bboxes, linds, flags, *args)\n",
    "\n",
    "        return img, bboxes, linds, flags, args\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class Compose(_DatasetBase):\n",
    "    def __init__(self, datasets, **kwargs):\n",
    "        \"\"\"\n",
    "        :param datasets: tuple of Dataset\n",
    "        :param kwargs:\n",
    "            :param ignore:\n",
    "            :param transform:\n",
    "            :param target_transform:\n",
    "            :param augmentation:\n",
    "        \"\"\"\n",
    "        self.transform = kwargs.get('transform', None)\n",
    "        self.target_transform = kwargs.get('target_transform', None)\n",
    "        self.augmentation = kwargs.get('augmentation', None)\n",
    "\n",
    "        datasets = _check_ins('datasets', datasets, (tuple, list))\n",
    "\n",
    "        _datasets, _lens = [], []\n",
    "        _class_labels = None\n",
    "        for dataset in datasets:\n",
    "            try:\n",
    "                dataset = dataset(**kwargs)\n",
    "            except Exception as e:\n",
    "                raise ValueError('Invalid arguments were passed. {} could not be initialized because\\n{}'.format(dataset.__name__, e))\n",
    "            dataset = _check_ins('element of datasets', dataset, _DatasetBase)\n",
    "            if _class_labels is None:\n",
    "                _class_labels = dataset.class_labels\n",
    "            else:\n",
    "                #if set(_class_labels) != set(dataset.class_labels):\n",
    "                if _class_labels != dataset.class_labels:\n",
    "                    raise ValueError('all of datasets must be same class labels')\n",
    "\n",
    "            # initialization\n",
    "            _datasets += [dataset]\n",
    "\n",
    "            _lens += [len(_datasets[-1])]\n",
    "\n",
    "        self.datasets = _datasets\n",
    "        self.lens = _lens\n",
    "        self._class_labels = _class_labels\n",
    "\n",
    "    @property\n",
    "    def class_labels(self):\n",
    "        return self._class_labels\n",
    "    @property\n",
    "    def class_nums(self):\n",
    "        return len(self._class_labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        for i in range(len(self.lens)):\n",
    "            if index < sum(self.lens[:i+1]):\n",
    "                return self.datasets[i][index - sum(self.lens[:i])]\n",
    "\n",
    "        raise ValueError('Index out of range')\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c32a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def batch_ind_fn(batch):\n",
    "    \"\"\"\n",
    "    :param batch:\n",
    "    :return:\n",
    "        imgs: Tensor, shape = (b, c, h, w)\n",
    "        targets: list of Tensor, whose shape = (object box num, 4 + class num) including background\n",
    "    \"\"\"\n",
    "    imgs, gts = list(zip(*batch))\n",
    "\n",
    "    return torch.stack(imgs), gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e77e29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, fnmatch\n",
    "import numpy as np\n",
    "\n",
    "def _get_recurrsive_paths(basedir, ext):\n",
    "    \"\"\"\n",
    "    :param basedir:\n",
    "    :param ext:\n",
    "    :return: list of path of files including basedir and ext(extension)\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for root, dirnames, filenames in os.walk(basedir):\n",
    "        for filename in fnmatch.filter(filenames, '*.{}'.format(ext)):\n",
    "            matches.append(os.path.join(root, filename))\n",
    "    return sorted(matches)\n",
    "\n",
    "\n",
    "def _get_xml_et_value(xml_et, key, rettype=str):\n",
    "    \"\"\"\n",
    "    :param xml_et: Elementtree's element\n",
    "    :param key:\n",
    "    :param rettype: class, force to convert it from str\n",
    "    :return: rettype's value\n",
    "    Note that if there is no keys in xml object, return None\n",
    "    \"\"\"\n",
    "    elm = xml_et.find(key)\n",
    "    if elm is None:\n",
    "        return elm\n",
    "\n",
    "    if isinstance(rettype, str):\n",
    "        return elm.text\n",
    "    else:\n",
    "        return rettype(elm.text)\n",
    "\n",
    "def _one_hot_encode(indices, class_num):\n",
    "    \"\"\"\n",
    "    :param indices: list of index\n",
    "    :param class_num:\n",
    "    :return: ndarray, relu_one-hot vectors\n",
    "    \"\"\"\n",
    "    size = len(indices)\n",
    "    one_hot = np.zeros((size, class_num))\n",
    "    one_hot[np.arange(size), indices] = 1\n",
    "    return one_hot\n",
    "\n",
    "def _separate_ignore(target_transform):\n",
    "    \"\"\"\n",
    "    Separate Ignore by target_transform\n",
    "    :param target_transform:\n",
    "    :return: ignore, target_transform\n",
    "    \"\"\"\n",
    "    if target_transform:\n",
    "        from .target_transforms import Ignore, Compose\n",
    "        if isinstance(target_transform, Ignore):\n",
    "            return target_transform, None\n",
    "\n",
    "        if not isinstance(target_transform, Compose):\n",
    "            return None, target_transform\n",
    "\n",
    "        # search existing target_transforms.Ignore in target_transform\n",
    "        new_target_transform = []\n",
    "        ignore = None\n",
    "        for t in target_transform.target_transforms:\n",
    "            if isinstance(t, Ignore):\n",
    "                ignore = t\n",
    "            else:\n",
    "                new_target_transform += [t]\n",
    "        return ignore, Compose(new_target_transform)\n",
    "\n",
    "    else:\n",
    "        return None, target_transform\n",
    "\n",
    "def _contain_ignore(target_transform):\n",
    "    if target_transform:\n",
    "        from .target_transforms import Ignore, Compose\n",
    "        if isinstance(target_transform, Ignore):\n",
    "            raise ValueError('target_transforms.Ignore must be passed to \\'ignore\\' argument')\n",
    "\n",
    "        if isinstance(target_transform, Compose):\n",
    "            for t in target_transform.target_transforms:\n",
    "                if isinstance(t, Ignore):\n",
    "                    raise ValueError('target_transforms.Ignore must be passed to \\'ignore\\' argument')\n",
    "\n",
    "    return target_transform\n",
    "\n",
    "def _check_ins(name, val, cls, allow_none=False):\n",
    "    if allow_none and val is None:\n",
    "        return val\n",
    "\n",
    "    if not isinstance(val, cls):\n",
    "        raise ValueError('Argument \\'{}\\' must be {}, but got {}'.format(name, cls.__name__, type(val).__name__))\n",
    "    return val\n",
    "\n",
    "DATA_ROOT = os.path.join(os.path.expanduser('~'),'Desktop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "559e0294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, target_transforms):\n",
    "        self.target_transforms = target_transforms\n",
    "\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        for t in self.target_transforms:\n",
    "            bboxes, labels, flags, args = t(bboxes, labels, flags, *args)\n",
    "        return bboxes, labels, flags, args\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.target_transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        return torch.from_numpy(bboxes), torch.from_numpy(labels), flags, args\n",
    "\n",
    "class Corners2Centroids(object):\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        # bbox = [xmin, ymin, xmax, ymax] to [cx, cy, w, h]\n",
    "        bboxes = np.concatenate(((bboxes[:, 2:] + bboxes[:, :2]) / 2,\n",
    "                                 (bboxes[:, 2:] - bboxes[:, :2])), axis=1)\n",
    "\n",
    "        return bboxes, labels, flags, args\n",
    "\n",
    "class Corners2MinMax(object):\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        # bbox = [xmin, ymin, xmax, ymax] to [xmin, xmax, ymin, ymax]\n",
    "        bboxes = bboxes[:, np.array((0, 2, 1, 3))]\n",
    "\n",
    "        return bboxes, labels, flags, args\n",
    "\n",
    "class Centroids2Corners(object):\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        # bbox = [cx, cy, w, h] to [xmin, ymin, xmax, ymax]\n",
    "        bboxes = np.concatenate((bboxes[:, :2] - bboxes[:, 2:]/2,\n",
    "                                 bboxes[:, :2] + bboxes[:, 2:]/2), axis=1)\n",
    "\n",
    "        return bboxes, labels, flags, args\n",
    "\n",
    "class Centroids2MinMax(object):\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        # bbox = [cx, cy, w, h] to [xmin, xmax, ymin, ymax]\n",
    "        bboxes = np.concatenate((bboxes[:, 0] - bboxes[:, 2]/2,\n",
    "                                 bboxes[:, 0] + bboxes[:, 2]/2,\n",
    "                                 bboxes[:, 1] - bboxes[:, 3]/2,\n",
    "                                 bboxes[:, 1] + bboxes[:, 3]/2), axis=1)\n",
    "\n",
    "        return bboxes, labels, flags, args\n",
    "\n",
    "class MinMax2Centroids(object):\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        # bbox = [xmin, xmax, ymin, ymax] to [cx, cy, w, h]\n",
    "        bboxes = np.concatenate((bboxes[:, 0] + (bboxes[:, 1] - bboxes[:, 0])/2,\n",
    "                                 bboxes[:, 2] + (bboxes[:, 3] - bboxes[:, 2])/2,\n",
    "                                 bboxes[:, 1] - bboxes[:, 0],\n",
    "                                 bboxes[:, 3] - bboxes[:, 2]), axis=1)\n",
    "\n",
    "        return bboxes, labels, flags, args\n",
    "\n",
    "class MinMax2Corners(object):\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        # bbox = [xmin, xmax, ymin, ymax] to [xmin, ymin, xmax, ymax]\n",
    "        bboxes = bboxes[:, np.array((0, 2, 1, 3))]\n",
    "\n",
    "        return bboxes, labels, flags, args\n",
    "\n",
    "class Ignore(object):\n",
    "    supported_key = ['difficult', 'truncated', 'occluded', 'iscrowd']\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        :param kwargs: if true, specific keyword will be ignored\n",
    "        \"\"\"\n",
    "        self.ignore_key = []\n",
    "        for key, val in kwargs.items():\n",
    "            if key in Ignore.supported_key:\n",
    "                val = _check_ins(key, val, bool)\n",
    "                if not val:\n",
    "                    logging.warning('No meaning: {}=False'.format(key))\n",
    "                else:\n",
    "                    self.ignore_key += [key]\n",
    "            else:\n",
    "                logging.warning('Unsupported arguments: {}'.format(key))\n",
    "\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        ret_bboxes = []\n",
    "        ret_labels = []\n",
    "        ret_flags = []\n",
    "\n",
    "        for bbox, label, flag in zip(bboxes, labels, flags):\n",
    "            flag_keys = list(flag.keys())\n",
    "            ig_flag = [flag[ig_key] if ig_key in flag_keys else False for ig_key in self.ignore_key]\n",
    "            if any(ig_flag):\n",
    "                continue\n",
    "            \"\"\"\n",
    "            isIgnore = False\n",
    "            for key, value in self.kwargs.items():\n",
    "                if value and key in flag and flag[key]:\n",
    "                    isIgnore = True\n",
    "                    break\n",
    "            if isIgnore:\n",
    "                continue\n",
    "            #if self._ignore_partial and flag['partial']:\n",
    "            #    continue\n",
    "            \"\"\"\n",
    "            # normalize\n",
    "            # bbox = [xmin, ymin, xmax, ymax]\n",
    "            ret_bboxes += [bbox]\n",
    "            ret_labels += [label]\n",
    "            ret_flags += [flag]\n",
    "\n",
    "        ret_bboxes = np.array(ret_bboxes, dtype=np.float32)\n",
    "        ret_labels = np.array(ret_labels, dtype=np.float32)\n",
    "\n",
    "        return ret_bboxes, ret_labels, ret_flags, args\n",
    "\n",
    "class OneHot(object):\n",
    "    def __init__(self, class_nums, add_background=True):\n",
    "        self._class_nums = class_nums\n",
    "        self._add_background = add_background\n",
    "        if add_background:\n",
    "            self._class_nums += 1\n",
    "\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        if labels.ndim != 1:\n",
    "            raise ValueError('labels might have been already relu_one-hotted or be invalid shape')\n",
    "\n",
    "        labels = _one_hot_encode(labels.astype(np.int), self._class_nums)\n",
    "        labels = np.array(labels, dtype=np.float32)\n",
    "\n",
    "        return bboxes, labels, flags, args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f625626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, target_transforms):\n",
    "        self.target_transforms = target_transforms\n",
    "\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        for t in self.target_transforms:\n",
    "            bboxes, labels, flags, args = t(bboxes, labels, flags, *args)\n",
    "        return bboxes, labels, flags, args\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.target_transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        return torch.from_numpy(bboxes), torch.from_numpy(labels), flags, args\n",
    "\n",
    "class Corners2Centroids(object):\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        # bbox = [xmin, ymin, xmax, ymax] to [cx, cy, w, h]\n",
    "        bboxes = np.concatenate(((bboxes[:, 2:] + bboxes[:, :2]) / 2,\n",
    "                                 (bboxes[:, 2:] - bboxes[:, :2])), axis=1)\n",
    "\n",
    "        return bboxes, labels, flags, args\n",
    "\n",
    "class Corners2MinMax(object):\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        # bbox = [xmin, ymin, xmax, ymax] to [xmin, xmax, ymin, ymax]\n",
    "        bboxes = bboxes[:, np.array((0, 2, 1, 3))]\n",
    "\n",
    "        return bboxes, labels, flags, args\n",
    "\n",
    "class Centroids2Corners(object):\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        # bbox = [cx, cy, w, h] to [xmin, ymin, xmax, ymax]\n",
    "        bboxes = np.concatenate((bboxes[:, :2] - bboxes[:, 2:]/2,\n",
    "                                 bboxes[:, :2] + bboxes[:, 2:]/2), axis=1)\n",
    "\n",
    "        return bboxes, labels, flags, args\n",
    "\n",
    "class Centroids2MinMax(object):\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        # bbox = [cx, cy, w, h] to [xmin, xmax, ymin, ymax]\n",
    "        bboxes = np.concatenate((bboxes[:, 0] - bboxes[:, 2]/2,\n",
    "                                 bboxes[:, 0] + bboxes[:, 2]/2,\n",
    "                                 bboxes[:, 1] - bboxes[:, 3]/2,\n",
    "                                 bboxes[:, 1] + bboxes[:, 3]/2), axis=1)\n",
    "\n",
    "        return bboxes, labels, flags, args\n",
    "\n",
    "class MinMax2Centroids(object):\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        # bbox = [xmin, xmax, ymin, ymax] to [cx, cy, w, h]\n",
    "        bboxes = np.concatenate((bboxes[:, 0] + (bboxes[:, 1] - bboxes[:, 0])/2,\n",
    "                                 bboxes[:, 2] + (bboxes[:, 3] - bboxes[:, 2])/2,\n",
    "                                 bboxes[:, 1] - bboxes[:, 0],\n",
    "                                 bboxes[:, 3] - bboxes[:, 2]), axis=1)\n",
    "\n",
    "        return bboxes, labels, flags, args\n",
    "\n",
    "class MinMax2Corners(object):\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        # bbox = [xmin, xmax, ymin, ymax] to [xmin, ymin, xmax, ymax]\n",
    "        bboxes = bboxes[:, np.array((0, 2, 1, 3))]\n",
    "\n",
    "        return bboxes, labels, flags, args\n",
    "\n",
    "class Ignore(object):\n",
    "    supported_key = ['difficult', 'truncated', 'occluded', 'iscrowd']\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        :param kwargs: if true, specific keyword will be ignored\n",
    "        \"\"\"\n",
    "        self.ignore_key = []\n",
    "        for key, val in kwargs.items():\n",
    "            if key in Ignore.supported_key:\n",
    "                val = _check_ins(key, val, bool)\n",
    "                if not val:\n",
    "                    logging.warning('No meaning: {}=False'.format(key))\n",
    "                else:\n",
    "                    self.ignore_key += [key]\n",
    "            else:\n",
    "                logging.warning('Unsupported arguments: {}'.format(key))\n",
    "\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        ret_bboxes = []\n",
    "        ret_labels = []\n",
    "        ret_flags = []\n",
    "\n",
    "        for bbox, label, flag in zip(bboxes, labels, flags):\n",
    "            flag_keys = list(flag.keys())\n",
    "            ig_flag = [flag[ig_key] if ig_key in flag_keys else False for ig_key in self.ignore_key]\n",
    "            if any(ig_flag):\n",
    "                continue\n",
    "            \"\"\"\n",
    "            isIgnore = False\n",
    "            for key, value in self.kwargs.items():\n",
    "                if value and key in flag and flag[key]:\n",
    "                    isIgnore = True\n",
    "                    break\n",
    "            if isIgnore:\n",
    "                continue\n",
    "            #if self._ignore_partial and flag['partial']:\n",
    "            #    continue\n",
    "            \"\"\"\n",
    "            # normalize\n",
    "            # bbox = [xmin, ymin, xmax, ymax]\n",
    "            ret_bboxes += [bbox]\n",
    "            ret_labels += [label]\n",
    "            ret_flags += [flag]\n",
    "\n",
    "        ret_bboxes = np.array(ret_bboxes, dtype=np.float32)\n",
    "        ret_labels = np.array(ret_labels, dtype=np.float32)\n",
    "\n",
    "        return ret_bboxes, ret_labels, ret_flags, args\n",
    "\n",
    "class OneHot(object):\n",
    "    def __init__(self, class_nums, add_background=True):\n",
    "        self._class_nums = class_nums\n",
    "        self._add_background = add_background\n",
    "        if add_background:\n",
    "            self._class_nums += 1\n",
    "\n",
    "    def __call__(self, bboxes, labels, flags, *args):\n",
    "        if labels.ndim != 1:\n",
    "            raise ValueError('labels might have been already relu_one-hotted or be invalid shape')\n",
    "\n",
    "        labels = _one_hot_encode(labels.astype(np.int), self._class_nums)\n",
    "        labels = np.array(labels, dtype=np.float32)\n",
    "\n",
    "        return bboxes, labels, flags, args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc264974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import logging\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes, labels, flags, *args):\n",
    "        for t in self.transforms:\n",
    "            transformed = t(img, bboxes, labels, flags, *args)\n",
    "            img, bboxes, labels, flags = transformed[:4]\n",
    "            args = transformed[4:]\n",
    "        return img, bboxes, labels, flags, args\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string\n",
    "\n",
    "\"\"\"\n",
    "bellow classes are consisted of\n",
    "    :param img: Tensor\n",
    "    :param bboxes: ndarray of bboxes\n",
    "    :param labels: ndarray of bboxes' indices\n",
    "    :param flags: list of flag's dict\n",
    "    :return: Tensor of img, ndarray of bboxes, ndarray of labels, dict of flags\n",
    "\"\"\"\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"\n",
    "    Note that convert ndarray to tensor and [0-255] to [0-1]\n",
    "    \"\"\"\n",
    "    def __call__(self, img, *args):\n",
    "        # convert ndarray into Tensor\n",
    "        # transpose img's tensor (h, w, c) to pytorch's format (c, h, w). (num, c, h, w)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        return (torch.from_numpy(img).float() / 255., *args)\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        :param size: 2d-array-like, (height, width)\n",
    "        \"\"\"\n",
    "        self._size = size\n",
    "\n",
    "    def __call__(self, img, *args):\n",
    "        return (cv2.resize(img, self._size), *args)\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    #def __init__(self, rgb_means=(103.939, 116.779, 123.68), rgb_stds=(1.0, 1.0, 1.0)):\n",
    "    def __init__(self, rgb_means=(0.485, 0.456, 0.406), rgb_stds=(0.229, 0.224, 0.225)):\n",
    "        self.means = np.array(rgb_means, dtype=np.float32).reshape((-1, 1, 1))\n",
    "        if np.any(np.abs(self.means) > 1):\n",
    "            logging.warning(\"In general, mean value should be less than 1 because img's range is [0-1]\")\n",
    "\n",
    "        self.stds = np.array(rgb_stds, dtype=np.float32).reshape((-1, 1, 1))\n",
    "\n",
    "    def __call__(self, img, *args):\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            return ((img.float() - torch.from_numpy(self.means)) / torch.from_numpy(self.stds), *args)\n",
    "        else:\n",
    "            return ((img.astype(np.float32) - self.means) / self.stds, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ff4c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def batch_ind_fn(batch):\n",
    "    \"\"\"\n",
    "    :param batch:\n",
    "    :return:\n",
    "        imgs: Tensor, shape = (b, c, h, w)\n",
    "        targets: list of Tensor, whose shape = (object box num, 4 + class num) including background\n",
    "    \"\"\"\n",
    "    imgs, gts = list(zip(*batch))\n",
    "\n",
    "    return torch.stack(imgs), gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1320c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\takayanagi\\\\jjjkkk\\\\pytorch_SSD', 'C:\\\\Users\\\\takayanagi\\\\jjjkkk\\\\pytorch_SSD', 'c:\\\\opencv\\\\build\\\\python', 'C:\\\\Users\\\\takayanagi\\\\anaconda3\\\\envs\\\\jjjkkk\\\\python39.zip', 'C:\\\\Users\\\\takayanagi\\\\anaconda3\\\\envs\\\\jjjkkk\\\\DLLs', 'C:\\\\Users\\\\takayanagi\\\\anaconda3\\\\envs\\\\jjjkkk\\\\lib', 'C:\\\\Users\\\\takayanagi\\\\anaconda3\\\\envs\\\\jjjkkk', '', 'C:\\\\Users\\\\takayanagi\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages', 'C:\\\\Users\\\\takayanagi\\\\anaconda3\\\\envs\\\\jjjkkk\\\\lib\\\\site-packages', 'C:\\\\Users\\\\takayanagi\\\\anaconda3\\\\envs\\\\jjjkkk\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\takayanagi\\\\anaconda3\\\\envs\\\\jjjkkk\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\takayanagi\\\\anaconda3\\\\envs\\\\jjjkkk\\\\lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca661a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
